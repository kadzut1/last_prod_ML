# -*- coding: utf-8 -*-
"""–º–æ–¥–µ–ª—å.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/171F-wWAMVcA5S3YFVpdeny_OBsCAz0cE
"""

import os
import pandas as pd
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,AutoModelForQuestionAnswering,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  tokenizer = AutoTokenizer.from_pretrained("2KKLabs/Kaleidoscope_small_v1")
  model = AutoModelForQuestionAnswering.from_pretrained("2KKLabs/Kaleidoscope_small_v1")
  model.to(device)

  context = "–ê–ª—å–±–µ—Ä—Ç –≠–π–Ω—à—Ç–µ–π–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª —Ç–µ–æ—Ä–∏—é –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
  question = "–ö—Ç–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª —Ç–µ–æ—Ä–∏—é –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏?"

  inputs = tokenizer(question, context, return_tensors="pt", truncation=True, max_length=384)
  inputs = {k: v.to(device) for k, v in inputs.items()}
  outputs = model(**inputs)
  start_index = torch.argmax(outputs.start_logits)
  end_index = torch.argmax(outputs.end_logits)
  answer_tokens = inputs["input_ids"][0][start_index:end_index + 1]
  answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
  print("–û—Ç–≤–µ—Ç:", answer)

output_dir = "./chatbot-gpt-model"
if os.path.exists(output_dir) and not os.path.isdir(output_dir):
    os.remove(output_dir)
os.makedirs(output_dir, exist_ok=True)


df = pd.read_csv('/content/my_messages1.csv')[['text', 'target']].dropna().head(10000)
df['text_pair'] = '–í–≤–æ–¥: ' + df['text'] + '\n–û—Ç–≤–µ—Ç: ' + df['target']
dataset = Dataset.from_pandas(df[['text_pair']].rename(columns={'text_pair': 'text'}))

model_name = "sberbank-ai/rugpt3small_based_on_gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

def tokenize(example):
    return tokenizer(example['text'], truncation=True, max_length=512, padding="max_length")

tokenized_dataset = dataset.map(tokenize, batched=True)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

initial_params = {
    'learning_rate': 5e-5,
    'num_train_epochs': 3,
    'per_device_train_batch_size': 2,
}

max_attempts = 5
attempt = 0
previous_loss = float('inf')
min_improvement = 0.02
target_loss = 0.4

# –û–±—É—á–∞—é—â–∏–π —Ü–∏–∫–ª —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π
while attempt < max_attempts:
    print(f"\n –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {initial_params}")

    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=initial_params['num_train_epochs'],
        per_device_train_batch_size=initial_params['per_device_train_batch_size'],
        learning_rate=initial_params['learning_rate'],
        logging_steps=50,
        save_steps=500,
        save_total_limit=4,
        fp16=torch.cuda.is_available(),
        report_to="none",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    train_result = trainer.train()
    current_loss = train_result.training_loss

    print(f" Loss –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è: {current_loss:.4f}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –Ω—É–∂–Ω–æ–≥–æ loss
    if current_loss <= target_loss:
        print(f" –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª–µ–≤–∞—è –æ—à–∏–±–∫–∞: {current_loss:.4f} ‚â§ {target_loss}")


    if previous_loss - current_loss >= min_improvement:
        print(f" –£–ª—É—á—à–µ–Ω–∏–µ loss –Ω–∞ {previous_loss - current_loss:.4f}, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.")
        previous_loss = current_loss
        break
    else:
        print(" –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–ª—É—á—à–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ. –ú–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã...")
        initial_params['learning_rate'] *= 0.5
        initial_params['num_train_epochs'] += 1
        initial_params['per_device_train_batch_size'] = max(1, initial_params['per_device_train_batch_size'] - 1)
        previous_loss = current_loss
        attempt += 1

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –º–æ–¥–µ–ª—å
print(" –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(" –ì–æ—Ç–æ–≤–æ.")

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


model_path = "./chatbot-gpt-model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

def generate_response(prompt, max_new_tokens=100):
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.8,
            pad_token_id=tokenizer.eos_token_id
        )
    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
    return response.strip()

while True:
    user_input = input("üßë –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ '–≤—ã—Ö–æ–¥'): ")
    if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
        break
    prompt = f"–í–≤–æ–¥: {user_input}\n–û—Ç–≤–µ—Ç:"
    reply = generate_response(prompt)
    print(f"ü§ñ {reply}")

import os
import zipfile
from tqdm import tqdm

def zip_folder_with_progress(folder_path, output_zip):
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –ø–∞–ø–∫–∏
    file_paths = []
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            file_paths.append(os.path.join(root, file))

    with zipfile.ZipFile(output_zip, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:
        for file in tqdm(file_paths, desc="Archiving", unit="file"):
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –≤ –∞—Ä—Ö–∏–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
            arcname = os.path.relpath(file, folder_path)
            zipf.write(file, arcname)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
folder_to_zip = '/content/chatbot-gpt-model'
output_zip_file = '/content/chatbot-gpt-model.zip'

zip_folder_with_progress(folder_to_zip, output_zip_file)

from google.colab import drive
drive.mount('/content/drive')

!pip install -q google-cloud-storage
!gcloud auth login

from google.colab import drive
from tqdm import tqdm
import os

# –ü–æ–¥–∫–ª—é—á–∞–µ–º Google –î–∏—Å–∫
drive.mount('/content/drive')

# –ü—É—Ç–∏
source_path = "/content/chatbot-gpt-model.zip"
destination_path = "/content/drive/MyDrive/chatbot-gpt-model.zip"

# –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
file_size = os.path.getsize(source_path)

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
chunk_size = 1024 * 1024  # 1MB

with open(source_path, 'rb') as src, open(destination_path, 'wb') as dst:
    with tqdm(total=file_size, unit='B', unit_scale=True, desc="–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ") as pbar:
        while True:
            chunk = src.read(chunk_size)
            if not chunk:
                break
            dst.write(chunk)
            pbar.update(len(chunk))

print("‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –Ω–∞ Google –î–∏—Å–∫!")